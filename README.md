# mount

this is a repository recording papars, lectures or other knowledge I learned (starting from 19, Nov 11)

## papers
19, Nov 12
* [Orthogonal Gradient Descent for Continual Learning](https://arxiv.org/abs/1910.07104)
* [Task2Vec: Task Embedding for Meta-Learning](https://arxiv.org/abs/1902.03545)
* [Progressive Neural Networks](https://arxiv.org/abs/1606.04671)
* [The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks](https://arxiv.org/abs/1803.03635)
* [Rethinking the Value of Network Pruning](https://arxiv.org/abs/1810.05270)
* [Distilling the Knowledge in a Neural Network](https://arxiv.org/abs/1503.02531)

19, Nov 18
* [Gradient-based Hyperparameter Optimization through Reversible Learning](https://arxiv.org/abs/1502.03492)
* [Scalable Bayesian Optimization Using Deep Neural Networks](https://arxiv.org/abs/1502.05700)
* [TADAM: Task dependent adaptive metric for improved few-shot learning](https://arxiv.org/abs/1805.10123)
* [Meta-Learning with Differentiable Convex Optimization](https://arxiv.org/abs/1904.03758)
* [Meta-Dataset: A Dataset of Datasets for Learning to Learn from Few Examples](https://arxiv.org/abs/1903.03096)
* [Meta-Learning for Semi-Supervised Few-Shot Classification](https://arxiv.org/abs/1803.00676)
* [Semi-supervised Domain Adaptation via Minimax Entropy](https://arxiv.org/abs/1904.06487)
* [Adversarial Dropout Regularization](https://arxiv.org/abs/1711.01575)
* [Maximum Classifier Discrepancy for Unsupervised Domain Adaptation](https://arxiv.org/abs/1712.02560)

19, Nov 21
* [A General and Adaptive Robust Loss Function](https://arxiv.org/abs/1701.03077)
* [FiLM: Visual Reasoning with a General Conditioning Layer](https://arxiv.org/abs/1709.07871)
* [Rapid Adaptation with Conditionally Shifted Neurons](https://arxiv.org/abs/1712.09926)
* [Lifelong Learning with Dynamically Expandable Networks](https://arxiv.org/abs/1708.01547)
* [Neurogenesis deep learning: Extending deep networks to accommodate new classes](https://ieeexplore.ieee.org/document/7965898)
* [Reconciling meta-learning and continual learning with online mixtures of tasks](https://arxiv.org/abs/1812.06080)
* [Learn to Grow: A Continual Structure Learning Framework for Overcoming Catastrophic Forgetting](https://arxiv.org/abs/1904.00310)
* [Overcoming catastrophic forgetting in neural networks](https://arxiv.org/abs/1612.00796)
* [Functional Regularisation for Continual Learning](https://arxiv.org/abs/1901.11356)
* [Progress & Compress: A scalable framework for continual learning](https://arxiv.org/abs/1805.06370)
* [Improved Knowledge Distillation via Teacher Assistant: Bridging the Gap Between Student and Teacher](https://arxiv.org/abs/1902.03393)
* [Continual Learning Through Synaptic Intelligence](https://arxiv.org/abs/1703.04200)
* [Overcoming Catastrophic Forgetting by Incremental Moment Matching](https://arxiv.org/abs/1703.08475)
* [Memory Aware Synapses: Learning what (not) to forget](https://arxiv.org/abs/1711.09601)
* [Attention-Based Structural-Plasticity](https://arxiv.org/abs/1903.06070)
* [Overcoming Catastrophic Interference by Conceptors](https://arxiv.org/abs/1707.04853)
* [Continuous Learning of Context-dependent Processing in Neural Networks](https://arxiv.org/abs/1810.01256)


## lectures
19, Nov 11
* Next Step of Machine Learning (Hung-yi Lee, 2019)
* [ICCV 2019, Visual Learning with Limited Labeled Data](https://sites.google.com/view/learning-with-limited-data)

