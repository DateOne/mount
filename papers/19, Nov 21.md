# A General and Adaptive Robust Loss Function
## Pros
This paper introduces a more general loss function that can be adapted to several other loss 
functions with a parameter measuring robustness, which, according to the paper, can be 
learned (?). This paper also includes multiple loss functions this loss function can be 
adapted to and multiple common probability density functions the negative log-likelihood of
this loss function can be adapted to.
## Others
* in a certain sense, robustness is about the degree of a model being influenced by outliers 
than inliers.
* in this paper, the author explains different loss, focusing on their capability on handling
outliers, which inspires me that if I want to devise a model that can learn multiple distritions
(like an ensemble), diversity between domains (just like outliers) must be emphasized (by loss 
or by other tricks).
* this paper explains losses in terms of average, the theory of which renders me much thoughts
about understanding deep learning process visually.
* this paper's discussion about the neccessity of NLL is wonderful (the trade-off thing).
# FiLM: Visual Reasoning with a General Conditioning Layer
## Pros
This paper presents a FiLM model that specifies in a neural network learning activation (of 
each layer) manipulating parameters (in this specific task of visual reasoning, affine 
transformation parameters).
## Others
* The reason why affine transformation works might owes to the natural of the specific questions
in this visual reasoning dataset (colors, numbers or others). We could consider use other 
transformation manipulation parameters to learn dynamic structures for few-shot learning.
* visual reasoning is a very insteresting task. I want to learn more!
# Rapid Adaptation with Conditionally Shifted Neurons
## Pros
This paper improves MetaNet with conditional shifted neurons (didn't expect the task to be 
meta-learning).
## Cons
Too simple.
## Others
1. I should dig deeper into MetaNet. The fast weight seems to be fun.
2. Quick adaptation to tasks does have neuralscientific support!
# LifeLong Learning with Dynamically Expanable Networks
## Pros
In order to solve Life-long learning problems, instead of merely expanding neural networks 
(like progressive networks) or merely retraining parameters with no expanding (like EWC),
this paper utilize previously achieved information while still allowing the network capacity
to expand when necessary. This algorithm can be divded into three parts as selective
retraining, dynamic network expansion and network split/duplication.
## Cons
* Theory of selectively retraining differs slightly with some network pruning theories.
* Theory is wonderful, while methods can be persuasive, like will the method used in paper
to identify useful parameters be effective?
## Others
* Expansion while retaining is very inspiring.
* Spliting the neural network can be interesting.
## Neurogenesis Deep Learning, Extending deep networks to accommodate new classes
# Pros
This paper introduces a method to accommodate new classes in autoencoding.
# Cons
Pretty dumb method.
# Others
The medical background seems to be fun.
## Reconciling meta-learning and continual learning with online mixtures of tasks
This paper is kinda too hard. I will read this later.
# Overcoming catastrophic forgetting in neural networks
## Cons
Just that paper, you know, EWC.
## Others
I didn't notice fisher information in this paper. Fisher information matrix is so 
important.
# Progress & Compress: A scalable framework for continual learning
Note later.
