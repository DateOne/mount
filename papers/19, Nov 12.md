# Orthogonal Gradient Descent for Continual Learning
## Pros
This paper uses gradients of model (from formal tasks) to restrict the direction of new task's
gradients of loss. Simplicities like OGD-GTL have been employed in actual implementation.
Besides, the illustration of stochastic gradient descent and orthogonal gradient descent is
interesting.
## Cons
1. Don't it still has to perserve some data (of gradients)?
2. Benchmarking orthogonal gradient descent with SGD is interesting. But I'm sure there will 
be a more appropriate method to handle non-iid data.
3. how do we get gradients of model actually (with no loss)?
## Others
The related work section of this paper remains me how facinating is the development of
incremental learning. Definitely should check on that later.

# Task2Vec: Task Embedding to Meta-Learning
## Pros
1. This paper employs fisher information matrix associated with parameters of a ImageNet-pre-trained
model (or a 'probe network') to obtain the embedding of a given task. According to the author,
semantic and taxonomic relations among tasks can be derived.
2. bonus: encoding task difficulty; encoding difficult features for the task.
3. specially devised distances like taxonomic distance and transfer distance.
## Others
1. fisher information seems to be a very important concept which deserves my more attention and
studying.
2. Combined with other papers on task embedding, will it be possible that I devise a perfect task
embedding?

# Progressive Neural Networks
## Pros
This paper grows neural networks (by learning lateral connections) to leverage transfer and 
avoiding catastropic forgetting. It uses average pertubation sensitivity and average fisher 
sensibility to study transferred features, both of which being inspiring especially the one
about fisher information. Besides, it also makes some statements about knowledge
distillation and that relationship between tasks can be orthogonal and adversarial.
Definitely gonna dig into that!
## Others
Sadly, I can't understand fisher information. I will read more papers later.
I guess in this paper, fisher information is about measuring the sensitivity of loss to 
parameters. I guess fisher information should be of great importance to knowledge 
distillation. More papers in this area should be explored.

# The Lottery Ticket Hypothsis: Finding Sparse, Trainable Neural Networks
## Pros
A very very very interesting hypothesis
## Others
How can I win this lottery?

# Rethinking the Value of Network Pruning
## Others
Very toubling thinking of network pruning which ends up being contradictory to the last paper.
I might need more thinking about these two papers.

# Distilling Knowledge in a Neural Network
## Pros
This paper employs predictions learned from bigger models as objective function to distill 
knowledge into a smaller model.
## Cons
Not con but god this article in easy.
## Others
Objective function sa true objective function might be more important than we thought.

# unsolved problems:
1. Papers presented in Related Work section in Orthogonal Gradient Descent for Continual Learning
are worthy of my reading.
2. Fisher information exploration.
3. Thinking in perfect task embedding (of course you should read more papers).
4. Thinking in knowledge distillation (or model compression), model expansion, neural parameters
search (the method that quickly form a neural network) and the relationship between ensemble and
my idea.
